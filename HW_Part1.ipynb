{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK 4\n",
    "## 1) Does basic house information reflect house's description?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework our task is to perform a clustering analysis of house announcements in Rome from Immobiliare.it. We will create two datasets by retrieving data from websites. Then, we will\n",
    "implement two clustering methods and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import collect_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scraping data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, we are collecting the list of interesting urls, starting from the website:\n",
    "https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=1.\n",
    "\n",
    "+ We are using the time.sleep function to prevent the websie blocking.\n",
    "+ The implementation of \"thread_find_links\" function is located in the external file **collect_data.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_threads = []\n",
    "for nr_page in range(1, 100):\n",
    "    t = threading.Thread(target=collect_data.thread_find_links, args=(\"nr_page\",))\n",
    "    t.start()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We are saving collected links into **links.txt** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links.txt\", \"w\") as link_file:\n",
    "    for url_ in link_threads:\n",
    "        link_file.write(url_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from link.txt \n",
    "with open(\"links.txt\", \"r\") as link_file:\n",
    "    link_threads = link_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We are iterating through collected links to retrieve and save data about flats. \n",
    "+ For this, we defined the fuction **threadExtract**. \n",
    "+ We applied the parallel programming by generating threads for each url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threadExtract(url):\n",
    "    \"\"\"\n",
    "    This function gets html content and scraps the interesting data\n",
    "    Input: String: \"url\" \n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    scrapped_data = collect_data.scrap_data(soup)\n",
    "    if scrapped_data != None:\n",
    "        collected_data.append(scrapped_data)\n",
    "        datawriter.writerow(scrapped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The collected data are saved in **flat_data.txt** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open(\"flat_data.txt\", \"w\", encoding='utf-8')\n",
    "headers = [\"price\", \"locali\", \"superficie\", \"bagni\", \"piano\", \"description\"]\n",
    "datawriter = csv.writer(data_file)\n",
    "datawriter.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "failed_links = []\n",
    "collected_data = []\n",
    "for link_ in link_threads:\n",
    "    try:\n",
    "        threading.Thread(threadExtract(link_[:-1])).start() #-1 because we have \"\\n\" in the end of a link\n",
    "        time.sleep(2) #for non-blocking\n",
    "        count += 1\n",
    "        if count >= 10000:break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        failed_links.append(link_)\n",
    "        \n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Reading the **flat_data.txt** file content as dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"flat_data.txt\", \"r\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
