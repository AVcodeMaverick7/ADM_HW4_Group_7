{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping data (old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen as ureq\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns =['price', 'locali', 'superficie', 'bagni','piano'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "for i in range(1,10):\n",
    "    content = requests.get(\"https://www.immobiliare.it/vendita-case/roma/?criterio=rilevanza&pag=\"+str(i))\n",
    "    soup = BeautifulSoup(content.text, 'html.parser')\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        url = link['href']\n",
    "        if url.startswith('https://www.immobiliare.it/') and url.endswith('.html'):\n",
    "            links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = requests.get(links[0])\n",
    "soup = BeautifulSoup(content.text, 'html.parser')\n",
    "features = soup.find_all(class_=[\"list-inline list-piped features__list\"])[0].get_text()\n",
    "features = features.replace('\\xa0', \"\")\n",
    "tmp = features.split(\"locali\")\n",
    "locali = int(tmp[0])\n",
    "superficie = int(tmp[1].split('m2superficie')[0]) \n",
    "bagni, piano = map(int, tmp[1].split('m2superficie')[1].split()[0].split(\"bagni\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adm_hm4'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "client = MongoClient()\n",
    "my_key = \"AnqLqo5TT9JDfzfqKrDUEYUqJ6HWZJy8\"\n",
    "base = \"https://api.mlab.com/api/1\"\n",
    "url_db = base + \"/databases?apiKey=\" + my_key\n",
    "response = requests.get(url_db)\n",
    "databases = json.loads(response.text)\n",
    "adm_database = databases[0]\n",
    "adm_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure\n",
    "headers = {'content-type':'application/json'}\n",
    "\n",
    "# test data\n",
    "flat_data = [{'price':100,'locali': 2, 'superficie': 1, \"bagni\": 2, \"piano\": 5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I manually created the collection 'flats' in \"adm_hm4\" database\n",
    "collection = 'flats'\n",
    "url = base + '/databases/' + adm_database + \"/collections/\" + collection +'/?apiKey=' + my_key\n",
    "\n",
    "# Pushing data into database\n",
    "payload = json.dumps(flat_data)\n",
    "response = requests.post(url, data=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': {'$oid': '5bffffcb1f6e4f34a8ba7023'},\n",
       "  'bagni': 2,\n",
       "  'locali': 2,\n",
       "  'piano': 5,\n",
       "  'price': 100,\n",
       "  'superficie': 1}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking what is inside our database\n",
    "response = requests.get(url)\n",
    "result = json.loads(response.text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to extract data from websites and push into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extractInfo is function to scrap html ; output --> dictionary\n",
    "import logging\n",
    "\n",
    "def threadExtract(url):\n",
    "    logging.info('extracting' + url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html5lib\")\n",
    "    work = extractInfo(soup) #output should be a dictionary\n",
    "    \n",
    "    # here: putting into csv or mongo_db\n",
    "    # MONGO_DB structure--> [{'price':100,'locali': 2, 'superficie': 1, \"bagni\": 2, \"piano\": 5}]\n",
    "    \n",
    "    logging.info(\"Done extracting\")\n",
    "    \n",
    "for url in links:\n",
    "    threadExtract(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
